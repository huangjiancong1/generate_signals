{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AutoEncoding with force_torque_sensor use batch normalize\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "Datasets paper: https://arxiv.org/pdf/1807.06749.pdf\n",
    "\n",
    "Download: https://ibm.ent.box.com/s/vw4y576xlz6ujblpl3gz9c5ttu51qc18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "sides_3 = np.load('../data/force_torque_sensor/Dataset/3_sides/Data/data.npy')\n",
    "# sides_4 = np.load('../data/force_torque_sensor/Dataset/4_sides/Data/data.npy')\n",
    "# sides_5 = np.load('../data/force_torque_sensor/Dataset/5_sides/Data/data.npy')\n",
    "# sides_6 = np.load('../data/force_torque_sensor/Dataset/6_sides/Data/data.npy')\n",
    "# sides_200 = np.load('../data/force_torque_sensor/Dataset/200_sides/Data/data.npy')\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.plotly as py\n",
    "# import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.59982571e-03 8.41197524e-03 1.84235611e-04 8.74299074e-02\n",
      " 4.72123347e+00 8.83503356e+00 1.77576665e+00 3.23750009e-01\n",
      " 6.40706637e-01 1.52313336e-01 1.58400000e+03 1.00096209e+01]\n",
      "[-8.56557263e-03 -8.47084910e-03 -1.83196547e-04 -8.74442646e-02\n",
      " -7.90796674e+00 -3.74110003e+00 -3.83027245e+01 -1.48956004e+00\n",
      " -1.10945995e+00 -2.19713331e-01  1.00000000e+00  5.38825989e-05]\n"
     ]
    }
   ],
   "source": [
    "print(sides_3.max(axis=0))\n",
    "\n",
    "print(sides_3.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    0:\"Force x\",\n",
    "    1:\"Force y\",\n",
    "    2:\"Force z\",\n",
    "    3:\"Moment x\",\n",
    "    4:\"Moment y\",\n",
    "    5:\"Moment z\",\n",
    "    6:\"Peg Position x\",\n",
    "    7:\"Peg Position y\",\n",
    "    8:\"Peg Position z\",\n",
    "    9:\"Angle\",\n",
    "    10:\"Time\",\n",
    "    11:\"Counter\",\n",
    "}\n",
    "\n",
    "# data = []\n",
    "# sides=sides_200\n",
    "\n",
    "# for i in range(len(sides[0])):\n",
    "#     k=0\n",
    "#     samples=[]\n",
    "    \n",
    "#     for j in range(15853):\n",
    "#         samples.append(sides[k][i])\n",
    "#         k+=100\n",
    "        \n",
    "#     trace0 = go.Scatter(\n",
    "#         x=np.linspace(0, len(samples)-1, num=len(samples), endpoint=True),\n",
    "#         y=samples,\n",
    "#         mode='lines+markers',\n",
    "#         name=names[i],\n",
    "#         hoverinfo='name',\n",
    "        \n",
    "#         marker=dict(\n",
    "#                 size=3.5,\n",
    "#         ),\n",
    "#         line=dict(\n",
    "#             shape='linear',\n",
    "# #             color=colors[i],\n",
    "#              width=0.4,\n",
    "#         )\n",
    "#     )\n",
    "#     data.append(trace0)\n",
    "\n",
    "# layout = dict(\n",
    "#     legend=dict(\n",
    "#         y=0.5,\n",
    "#         traceorder='reversed',\n",
    "#         font=dict(\n",
    "#             size=16\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "# fig = dict(data=data, layout=layout)\n",
    "# plotly.offline.init_notebook_mode()\n",
    "# plotly.offline.iplot(fig)\n",
    "\n",
    "\n",
    "# # py.iplot(fig, filename='force_torque_dataset_sides_200_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap=plt.cm.get_cmap(plt.cm.viridis,143)\n",
    "\n",
    "\n",
    "# plt.rcParams['font.size'] = 11.\n",
    "# plt.rcParams['font.family'] = 'Comic Sans MS'\n",
    "# plt.rcParams['axes.labelsize'] = 15.\n",
    "# plt.rcParams['xtick.labelsize'] = 10.\n",
    "# plt.rcParams['ytick.labelsize'] = 10.\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "\n",
    "\n",
    "sides = sides_3\n",
    "print(len(sides))\n",
    "\n",
    "Force_x = sides[:,0] \n",
    "Force_y = sides[:,1]\n",
    "Force_z = sides[:,2]\n",
    "Moment_x = sides[:,3]\n",
    "Moment_y = sides[:,4]\n",
    "Moment_z = sides[:,5]\n",
    "Peg_Position_x = sides[:,6]\n",
    "Peg_Position_y = sides[:,7]\n",
    "Peg_Position_z = sides[:,8]\n",
    "Angle = sides[:,9]\n",
    "Time = sides[:,10]\n",
    "Counter = sides[:,11]\n",
    "\n",
    "\n",
    "# plt.plot(Time, Force_x, marker='s', linestyle='-', markersize=2, linewidth=1, label='Force_x')\n",
    "plt.plot(Time, Force_y, marker='o', linestyle='-', markersize=2, linewidth=1, label='Force_y')\n",
    "# plt.plot(Time, Force_z, marker='o', linestyle='-', markersize=2, linewidth=1, label='Force_z')\n",
    "\n",
    "# plt.plot(Time, Moment_x, marker='o, linestyle='-', label='Moment_x')\n",
    "# plt.plot(Time, Moment_y, marker='o, linestyle='-', label='Moment_y')\n",
    "# plt.plot(Time, Moment_z, marker='o, linestyle='-', label='Moment_z')\n",
    "\n",
    "# plt.plot(Time, Peg_Position_x, marker='o', linestyle='-', label='Peg_Position_x')\n",
    "# plt.plot(Time, Peg_Position_y, marker='o', linestyle='-', label='Peg_Position_y')\n",
    "# plt.plot(Time, Peg_Position_z, marker='o', linestyle='-', label='Peg_Position_z')\n",
    "\n",
    "# plt.plot(Time, Angle, marker='o', linestyle='-', label='Angle')\n",
    "# plt.plot(Time, Counter, marker='o', linestyle='-', label='Counter')\n",
    "\n",
    "\n",
    "plt.xlabel('Times')\n",
    "plt.ylabel('Signals')\n",
    "plt.title('3 Sileds force torque signals')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585353\n",
      "12\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd8716c7518>\n"
     ]
    }
   ],
   "source": [
    "print (len(sides_3))\n",
    "print (len(sides_3[0]))\n",
    "\n",
    "import torch.utils.data\n",
    "train_loader = torch.utils.data.DataLoader(sides_3, batch_size=2000, num_workers=4)\n",
    "\n",
    "print (train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from models import AE\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5285\n"
     ]
    }
   ],
   "source": [
    "latent_size = 2\n",
    "\n",
    "sides_3 = np.load('../data/force_torque_sensor/Dataset/3_sides/Data/data.npy')\n",
    "\n",
    "# train_data = torch.from_numpy(sides_3).double()\n",
    "\n",
    "# X = Variable(X).float()\n",
    "\n",
    "\n",
    "from torch.utils.data.sampler import  WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=9, replacement=True)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=3,\n",
    "                        sampler=sampler)\n",
    "\n",
    "\n",
    "import torch.utils.data\n",
    "train_loader = torch.utils.data.DataLoader(sides_3, batch_size=300, num_workers=4)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Encoder(\n",
      "    (fc1): Linear(in_features=12, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (batch_norm1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc3): Linear(in_features=2, out_features=8, bias=True)\n",
      "    (fc4): Linear(in_features=8, out_features=12, bias=True)\n",
      "    (batch_norm3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ae = AE(latent_size=latent_size).float()\n",
    "\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "\n",
    "logs = defaultdict(list)\n",
    "print(ae)\n",
    "\n",
    "use_cuda = torch.cuda.is_available\n",
    "if use_cuda:\n",
    "    ae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "def loss_fn(recon_x, x):\n",
    "    criterion = nn.MSELoss()\n",
    "#     x = x.long()\n",
    "    loss = criterion(recon_x, x)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 00/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 00/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 00/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 00/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 00/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 00/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 00/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 00/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 00/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 00/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 00/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 00/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 00/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 00/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 00/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 00/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 00/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 00/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 00/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 00/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 00/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 00/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 00/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 00/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 00/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 00/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 00/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 00/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 00/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 00/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 00/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 00/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 00/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 00/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 00/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 00/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 00/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 00/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 00/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 00/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 00/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 00/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 00/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 00/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 00/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 00/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 00/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 00/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 00/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 00/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 00/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 00/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 00/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 01/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 01/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 01/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 01/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 01/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 01/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 01/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 01/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 01/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 01/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 01/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 01/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 01/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 01/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 01/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 01/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 01/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 01/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 01/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 01/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 01/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 01/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 01/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 01/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 01/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 01/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 01/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 01/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 01/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 01/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 01/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 01/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 01/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 01/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 01/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 01/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 01/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 01/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 01/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 01/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 01/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 01/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 01/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 01/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 01/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 01/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 01/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 01/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 01/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 01/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 01/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 01/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 01/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 01/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 02/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 02/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 02/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 02/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 02/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 02/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 02/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 02/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 02/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 02/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 02/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 02/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 02/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 02/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 02/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 02/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 02/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 02/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 02/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 02/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 02/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 02/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 02/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 02/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 02/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 02/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 02/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 02/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 02/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 02/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 02/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 02/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 02/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 02/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 02/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 02/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 02/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 02/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 02/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 02/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 02/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 02/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 02/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 02/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 02/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 02/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 02/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 02/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 02/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 02/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 02/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 02/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 02/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 02/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 03/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 03/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 03/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 03/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 03/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 03/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 03/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 03/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 03/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 03/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 03/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 03/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 03/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 03/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 03/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 03/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 03/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 03/100 Batch 1700/5284, Loss 9622.8799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 03/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 03/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 03/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 03/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 03/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 03/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 03/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 03/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 03/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 03/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 03/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 03/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 03/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 03/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 03/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 03/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 03/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 03/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 03/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 03/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 03/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 03/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 03/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 03/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 03/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 03/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 03/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 03/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 03/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 03/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 03/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 03/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 03/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 03/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 03/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 04/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 04/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 04/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 04/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 04/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 04/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 04/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 04/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 04/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 04/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 04/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 04/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 04/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 04/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 04/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 04/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 04/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 04/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 04/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 04/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 04/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 04/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 04/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 04/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 04/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 04/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 04/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 04/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 04/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 04/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 04/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 04/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 04/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 04/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 04/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 04/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 04/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 04/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 04/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 04/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 04/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 04/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 04/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 04/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 04/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 04/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 04/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 04/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 04/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 04/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 04/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 04/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 04/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 04/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 05/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 05/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 05/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 05/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 05/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 05/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 05/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 05/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 05/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 05/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 05/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 05/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 05/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 05/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 05/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 05/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 05/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 05/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 05/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 05/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 05/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 05/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 05/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 05/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 05/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 05/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 05/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 05/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 05/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 05/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 05/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 05/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 05/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 05/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 05/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 05/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 05/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 05/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 05/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 05/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 05/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 05/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 05/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 05/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 05/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 05/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 05/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 05/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 05/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 05/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 05/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 05/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 05/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 05/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 06/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 06/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 06/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 06/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 06/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 06/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 06/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 06/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 06/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 06/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 06/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 06/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 06/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 06/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 06/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 06/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 06/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 06/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 06/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 06/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 06/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 06/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 06/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 06/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 06/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 06/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 06/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 06/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 06/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 06/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 06/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 06/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 06/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 06/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 06/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 06/100 Batch 3500/5284, Loss 3412.1672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 06/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 06/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 06/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 06/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 06/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 06/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 06/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 06/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 06/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 06/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 06/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 06/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 06/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 06/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 06/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 06/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 06/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 07/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 07/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 07/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 07/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 07/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 07/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 07/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 07/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 07/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 07/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 07/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 07/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 07/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 07/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 07/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 07/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 07/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 07/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 07/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 07/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 07/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 07/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 07/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 07/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 07/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 07/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 07/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 07/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 07/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 07/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 07/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 07/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 07/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 07/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 07/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 07/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 07/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 07/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 07/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 07/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 07/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 07/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 07/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 07/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 07/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 07/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 07/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 07/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 07/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 07/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 07/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 07/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 07/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 07/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 08/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 08/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 08/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 08/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 08/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 08/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 08/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 08/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 08/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 08/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 08/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 08/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 08/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 08/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 08/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 08/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 08/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 08/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 08/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 08/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 08/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 08/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 08/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 08/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 08/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 08/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 08/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 08/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 08/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 08/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 08/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 08/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 08/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 08/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 08/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 08/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 08/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 08/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 08/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 08/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 08/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 08/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 08/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 08/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 08/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 08/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 08/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 08/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 08/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 08/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 08/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 08/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 08/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 08/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 09/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 09/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 09/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 09/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 09/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 09/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 09/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 09/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 09/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 09/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 09/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 09/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 09/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 09/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 09/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 09/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 09/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 09/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 09/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 09/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 09/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 09/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 09/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 09/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 09/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 09/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 09/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 09/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 09/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 09/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 09/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 09/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 09/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 09/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 09/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 09/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 09/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 09/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 09/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 09/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 09/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 09/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 09/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 09/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 09/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 09/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 09/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 09/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 09/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 09/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 09/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 09/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 09/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 09/100 Batch 5284/5284, Loss 45163.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 10/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 10/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 10/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 10/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 10/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 10/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 10/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 10/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 10/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 10/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 10/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 10/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 10/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 10/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 10/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 10/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 10/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 10/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 10/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 10/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 10/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 10/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 10/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 10/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 10/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 10/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 10/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 10/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 10/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 10/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 10/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 10/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 10/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 10/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 10/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 10/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 10/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 10/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 10/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 10/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 10/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 10/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 10/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 10/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 10/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 10/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 10/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 10/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 10/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 10/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 10/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 10/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 10/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 11/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 11/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 11/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 11/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 11/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 11/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 11/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 11/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 11/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 11/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 11/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 11/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 11/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 11/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 11/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 11/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 11/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 11/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 11/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 11/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 11/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 11/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 11/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 11/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 11/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 11/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 11/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 11/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 11/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 11/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 11/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 11/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 11/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 11/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 11/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 11/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 11/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 11/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 11/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 11/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 11/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 11/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 11/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 11/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 11/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 11/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 11/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 11/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 11/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 11/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 11/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 11/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 11/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 11/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 12/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 12/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 12/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 12/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 12/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 12/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 12/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 12/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 12/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 12/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 12/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 12/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 12/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 12/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 12/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 12/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 12/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 12/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 12/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 12/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 12/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 12/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 12/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 12/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 12/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 12/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 12/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 12/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 12/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 12/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 12/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 12/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 12/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 12/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 12/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 12/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 12/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 12/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 12/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 12/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 12/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 12/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 12/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 12/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 12/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 12/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 12/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 12/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 12/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 12/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 12/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 12/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 12/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 12/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 13/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 13/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 13/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 13/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 13/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 13/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 13/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 13/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 13/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 13/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 13/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 13/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 13/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 13/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 13/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 13/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 13/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 13/100 Batch 1700/5284, Loss 9622.8799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 13/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 13/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 13/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 13/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 13/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 13/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 13/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 13/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 13/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 13/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 13/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 13/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 13/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 13/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 13/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 13/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 13/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 13/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 13/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 13/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 13/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 13/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 13/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 13/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 13/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 13/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 13/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 13/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 13/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 13/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 13/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 13/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 13/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 13/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 13/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 14/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 14/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 14/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 14/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 14/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 14/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 14/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 14/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 14/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 14/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 14/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 14/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 14/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 14/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 14/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 14/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 14/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 14/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 14/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 14/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 14/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 14/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 14/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 14/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 14/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 14/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 14/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 14/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 14/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 14/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 14/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 14/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 14/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 14/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 14/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 14/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 14/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 14/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 14/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 14/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 14/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 14/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 14/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 14/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 14/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 14/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 14/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 14/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 14/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 14/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 14/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 14/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 14/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 14/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 15/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 15/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 15/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 15/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 15/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 15/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 15/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 15/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 15/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 15/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 15/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 15/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 15/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 15/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 15/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 15/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 15/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 15/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 15/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 15/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 15/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 15/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 15/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 15/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 15/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 15/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 15/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 15/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 15/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 15/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 15/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 15/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 15/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 15/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 15/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 15/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 15/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 15/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 15/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 15/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 15/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 15/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 15/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 15/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 15/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 15/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 15/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 15/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 15/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 15/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 15/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 15/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 15/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 15/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 16/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 16/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 16/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 16/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 16/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 16/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 16/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 16/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 16/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 16/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 16/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 16/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 16/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 16/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 16/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 16/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 16/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 16/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 16/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 16/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 16/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 16/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 16/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 16/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 16/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 16/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 16/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 16/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 16/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 16/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 16/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 16/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 16/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 16/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 16/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 16/100 Batch 3500/5284, Loss 3412.1672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 16/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 16/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 16/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 16/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 16/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 16/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 16/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 16/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 16/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 16/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 16/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 16/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 16/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 16/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 16/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 16/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 16/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 17/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 17/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 17/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 17/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 17/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 17/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 17/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 17/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 17/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 17/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 17/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 17/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 17/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 17/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 17/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 17/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 17/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 17/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 17/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 17/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 17/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 17/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 17/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 17/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 17/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 17/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 17/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 17/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 17/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 17/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 17/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 17/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 17/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 17/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 17/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 17/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 17/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 17/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 17/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 17/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 17/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 17/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 17/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 17/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 17/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 17/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 17/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 17/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 17/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 17/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 17/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 17/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 17/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 17/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 18/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 18/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 18/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 18/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 18/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 18/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 18/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 18/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 18/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 18/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 18/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 18/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 18/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 18/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 18/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 18/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 18/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 18/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 18/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 18/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 18/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 18/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 18/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 18/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 18/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 18/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 18/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 18/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 18/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 18/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 18/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 18/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 18/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 18/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 18/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 18/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 18/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 18/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 18/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 18/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 18/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 18/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 18/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 18/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 18/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 18/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 18/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 18/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 18/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 18/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 18/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 18/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 18/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 18/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 19/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 19/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 19/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 19/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 19/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 19/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 19/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 19/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 19/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 19/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 19/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 19/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 19/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 19/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 19/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 19/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 19/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 19/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 19/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 19/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 19/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 19/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 19/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 19/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 19/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 19/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 19/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 19/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 19/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 19/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 19/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 19/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 19/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 19/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 19/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 19/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 19/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 19/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 19/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 19/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 19/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 19/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 19/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 19/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 19/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 19/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 19/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 19/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 19/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 19/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 19/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 19/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 19/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 19/100 Batch 5284/5284, Loss 45163.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 20/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 20/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 20/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 20/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 20/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 20/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 20/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 20/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 20/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 20/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 20/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 20/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 20/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 20/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 20/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 20/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 20/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 20/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 20/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 20/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 20/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 20/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 20/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 20/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 20/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 20/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 20/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 20/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 20/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 20/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 20/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 20/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 20/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 20/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 20/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 20/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 20/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 20/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 20/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 20/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 20/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 20/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 20/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 20/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 20/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 20/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 20/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 20/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 20/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 20/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 20/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 20/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 20/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 21/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 21/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 21/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 21/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 21/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 21/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 21/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 21/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 21/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 21/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 21/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 21/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 21/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 21/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 21/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 21/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 21/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 21/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 21/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 21/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 21/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 21/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 21/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 21/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 21/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 21/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 21/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 21/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 21/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 21/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 21/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 21/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 21/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 21/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 21/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 21/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 21/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 21/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 21/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 21/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 21/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 21/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 21/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 21/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 21/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 21/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 21/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 21/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 21/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 21/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 21/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 21/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 21/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 21/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 22/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 22/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 22/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 22/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 22/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 22/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 22/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 22/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 22/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 22/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 22/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 22/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 22/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 22/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 22/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 22/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 22/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 22/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 22/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 22/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 22/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 22/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 22/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 22/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 22/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 22/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 22/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 22/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 22/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 22/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 22/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 22/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 22/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 22/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 22/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 22/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 22/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 22/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 22/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 22/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 22/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 22/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 22/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 22/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 22/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 22/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 22/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 22/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 22/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 22/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 22/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 22/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 22/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 22/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 23/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 23/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 23/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 23/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 23/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 23/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 23/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 23/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 23/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 23/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 23/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 23/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 23/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 23/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 23/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 23/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 23/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 23/100 Batch 1700/5284, Loss 9622.8799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 23/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 23/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 23/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 23/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 23/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 23/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 23/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 23/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 23/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 23/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 23/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 23/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 23/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 23/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 23/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 23/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 23/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 23/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 23/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 23/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 23/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 23/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 23/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 23/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 23/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 23/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 23/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 23/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 23/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 23/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 23/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 23/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 23/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 23/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 23/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 24/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 24/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 24/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 24/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 24/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 24/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 24/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 24/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 24/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 24/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 24/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 24/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 24/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 24/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 24/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 24/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 24/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 24/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 24/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 24/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 24/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 24/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 24/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 24/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 24/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 24/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 24/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 24/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 24/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 24/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 24/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 24/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 24/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 24/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 24/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 24/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 24/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 24/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 24/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 24/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 24/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 24/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 24/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 24/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 24/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 24/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 24/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 24/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 24/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 24/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 24/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 24/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 24/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 24/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 25/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 25/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 25/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 25/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 25/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 25/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 25/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 25/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 25/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 25/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 25/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 25/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 25/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 25/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 25/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 25/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 25/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 25/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 25/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 25/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 25/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 25/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 25/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 25/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 25/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 25/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 25/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 25/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 25/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 25/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 25/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 25/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 25/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 25/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 25/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 25/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 25/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 25/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 25/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 25/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 25/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 25/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 25/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 25/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 25/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 25/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 25/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 25/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 25/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 25/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 25/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 25/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 25/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 25/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 26/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 26/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 26/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 26/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 26/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 26/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 26/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 26/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 26/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 26/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 26/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 26/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 26/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 26/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 26/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 26/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 26/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 26/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 26/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 26/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 26/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 26/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 26/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 26/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 26/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 26/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 26/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 26/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 26/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 26/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 26/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 26/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 26/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 26/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 26/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 26/100 Batch 3500/5284, Loss 3412.1672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 26/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 26/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 26/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 26/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 26/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 26/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 26/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 26/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 26/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 26/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 26/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 26/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 26/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 26/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 26/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 26/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 26/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 27/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 27/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 27/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 27/100 Batch 0300/5284, Loss 47871.7969\n",
      "Epoch 27/100 Batch 0400/5284, Loss 44174.8086\n",
      "Epoch 27/100 Batch 0500/5284, Loss 40629.4648\n",
      "Epoch 27/100 Batch 0600/5284, Loss 37227.9258\n",
      "Epoch 27/100 Batch 0700/5284, Loss 33980.3477\n",
      "Epoch 27/100 Batch 0800/5284, Loss 30874.8496\n",
      "Epoch 27/100 Batch 0900/5284, Loss 27930.4395\n",
      "Epoch 27/100 Batch 1000/5284, Loss 25122.7266\n",
      "Epoch 27/100 Batch 1100/5284, Loss 22479.1621\n",
      "Epoch 27/100 Batch 1200/5284, Loss 19964.4492\n",
      "Epoch 27/100 Batch 1300/5284, Loss 17593.3574\n",
      "Epoch 27/100 Batch 1400/5284, Loss 15377.1182\n",
      "Epoch 27/100 Batch 1500/5284, Loss 13308.3438\n",
      "Epoch 27/100 Batch 1600/5284, Loss 11395.0215\n",
      "Epoch 27/100 Batch 1700/5284, Loss 9622.8799\n",
      "Epoch 27/100 Batch 1800/5284, Loss 8011.5063\n",
      "Epoch 27/100 Batch 1900/5284, Loss 6532.5723\n",
      "Epoch 27/100 Batch 2000/5284, Loss 5226.9766\n",
      "Epoch 27/100 Batch 2100/5284, Loss 4051.5500\n",
      "Epoch 27/100 Batch 2200/5284, Loss 3034.5627\n",
      "Epoch 27/100 Batch 2300/5284, Loss 2167.9258\n",
      "Epoch 27/100 Batch 2400/5284, Loss 1448.6458\n",
      "Epoch 27/100 Batch 2500/5284, Loss  887.2372\n",
      "Epoch 27/100 Batch 2600/5284, Loss  469.9804\n",
      "Epoch 27/100 Batch 2700/5284, Loss  204.7461\n",
      "Epoch 27/100 Batch 2800/5284, Loss   75.8004\n",
      "Epoch 27/100 Batch 2900/5284, Loss  104.2094\n",
      "Epoch 27/100 Batch 3000/5284, Loss  293.8652\n",
      "Epoch 27/100 Batch 3100/5284, Loss  612.5198\n",
      "Epoch 27/100 Batch 3200/5284, Loss 1089.5413\n",
      "Epoch 27/100 Batch 3300/5284, Loss 1725.4164\n",
      "Epoch 27/100 Batch 3400/5284, Loss 2494.9836\n",
      "Epoch 27/100 Batch 3500/5284, Loss 3412.1672\n",
      "Epoch 27/100 Batch 3600/5284, Loss 4492.9697\n",
      "Epoch 27/100 Batch 3700/5284, Loss 5723.5605\n",
      "Epoch 27/100 Batch 3800/5284, Loss 7098.7598\n",
      "Epoch 27/100 Batch 3900/5284, Loss 8623.4346\n",
      "Epoch 27/100 Batch 4000/5284, Loss 10305.9033\n",
      "Epoch 27/100 Batch 4100/5284, Loss 12135.2666\n",
      "Epoch 27/100 Batch 4200/5284, Loss 14115.9619\n",
      "Epoch 27/100 Batch 4300/5284, Loss 16246.0078\n",
      "Epoch 27/100 Batch 4400/5284, Loss 18526.1562\n",
      "Epoch 27/100 Batch 4500/5284, Loss 20956.3574\n",
      "Epoch 27/100 Batch 4600/5284, Loss 23539.2695\n",
      "Epoch 27/100 Batch 4700/5284, Loss 26265.7207\n",
      "Epoch 27/100 Batch 4800/5284, Loss 29140.1406\n",
      "Epoch 27/100 Batch 4900/5284, Loss 32163.1875\n",
      "Epoch 27/100 Batch 5000/5284, Loss 35346.2344\n",
      "Epoch 27/100 Batch 5100/5284, Loss 38665.1094\n",
      "Epoch 27/100 Batch 5200/5284, Loss 42142.2930\n",
      "Epoch 27/100 Batch 5284/5284, Loss 45163.3047\n",
      "Epoch 28/100 Batch 0000/5284, Loss 59861.4570\n",
      "Epoch 28/100 Batch 0100/5284, Loss 55715.9453\n",
      "Epoch 28/100 Batch 0200/5284, Loss 51717.5234\n",
      "Epoch 28/100 Batch 0300/5284, Loss 47871.7969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9e19ad5be079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/clustering/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print_every = 100\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for iteration, d in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            input_data = d.cuda().float()\n",
    "        else:\n",
    "            input_data = d.float()\n",
    "        optimizer.zero_grad()\n",
    "        recon_data = ae(input_data)\n",
    "        loss = loss_fn(recon_data, input_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logs['loss'].append(loss.item())\n",
    "\n",
    "        if iteration % print_every == 0 or iteration == len(train_loader)-1:\n",
    "            print(\"Epoch {:02d}/{:02d} Batch {:04d}/{:d}, Loss {:9.4f}\".format(\n",
    "                epoch, epochs, iteration, len(train_loader)-1, loss.item()))\n",
    "\n",
    "#             x = ae.inference(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('encoder.fc1.weight', Parameter containing:\n",
      "tensor([[ 5.4618e-01, -1.0196e-01, -1.2658e+00, -7.0105e-02, -5.1694e-02,\n",
      "         -2.7498e-01, -6.2870e-01, -3.2909e-01, -7.4361e-02,  6.4544e-01,\n",
      "          7.9597e-02,  6.4699e-02],\n",
      "        [-4.3559e-02, -2.0185e-01, -1.9503e+00,  7.5140e-02,  5.9266e-02,\n",
      "          1.6101e-01, -3.2256e-01, -2.4070e-01,  1.5877e-02,  1.4606e-01,\n",
      "          1.7031e-01,  1.6983e-01],\n",
      "        [-4.6926e-01,  1.0751e-01,  2.0826e+00, -4.3747e-01, -1.7400e-02,\n",
      "          8.2499e-02, -1.0064e+00, -3.1924e-01,  1.1691e-02,  7.5976e-02,\n",
      "          1.0204e-02,  6.6305e-02],\n",
      "        [-4.2995e-01,  2.1527e-01,  6.9897e-01,  1.0398e-01, -8.2728e-03,\n",
      "         -1.4070e-02, -5.0400e-03, -5.3240e-02,  3.8158e-02, -9.0849e-02,\n",
      "          2.7976e-01,  1.4188e+00],\n",
      "        [ 9.1981e-02, -4.6751e-02, -2.7918e-01, -2.4451e-01,  3.6599e-02,\n",
      "          6.2954e-02, -7.4778e-03,  2.8745e-02,  3.8803e-02,  1.2937e-01,\n",
      "          4.4606e-03,  1.5451e+00],\n",
      "        [ 2.1249e-01, -2.1985e-01, -1.8458e+00, -1.6800e-01, -5.2778e-02,\n",
      "         -9.5347e-02, -5.2303e-01,  3.7019e-02, -6.1171e-03,  1.5327e-01,\n",
      "          1.7082e-01, -5.2465e-02],\n",
      "        [-1.9575e+00,  1.6656e-01, -4.4620e-01, -1.4456e+00, -2.6521e-03,\n",
      "         -4.4100e-03,  5.2656e-04, -5.3596e-02,  5.2255e-02, -1.0437e-01,\n",
      "          4.7306e-01, -2.4262e-01],\n",
      "        [-5.5830e-01, -1.9806e-01,  2.1391e+00, -9.3277e-02, -2.3228e-02,\n",
      "         -2.2414e-01, -6.3334e-01, -1.5047e-01,  2.3560e-01,  2.9351e-01,\n",
      "          5.8877e-02,  6.9405e-03]], device='cuda:0', requires_grad=True)), ('encoder.fc1.bias', Parameter containing:\n",
      "tensor([ 0.4955,  0.4740,  0.7560, -0.4032,  0.7714,  0.0931,  1.7196, -0.0581],\n",
      "       device='cuda:0', requires_grad=True)), ('encoder.fc2.weight', Parameter containing:\n",
      "tensor([[-0.0686,  0.1568, -0.5474,  0.2990, -0.6263,  0.2238, -0.1148, -0.1802],\n",
      "        [ 0.2562,  0.0232, -0.1686, -0.2688, -0.5521,  0.0938,  0.3089, -0.2292]],\n",
      "       device='cuda:0', requires_grad=True)), ('encoder.fc2.bias', Parameter containing:\n",
      "tensor([0.3672, 0.7536], device='cuda:0', requires_grad=True)), ('encoder.batch_norm1.weight', Parameter containing:\n",
      "tensor([ 0.1488,  0.7722,  0.5002,  1.5087, -0.0095,  0.8933,  0.3342,  0.2228],\n",
      "       device='cuda:0', requires_grad=True)), ('encoder.batch_norm1.bias', Parameter containing:\n",
      "tensor([ 0.2440,  0.6131, -0.6615,  0.0710, -0.8072,  0.6614, -0.4049, -1.1992],\n",
      "       device='cuda:0', requires_grad=True)), ('encoder.batch_norm2.weight', Parameter containing:\n",
      "tensor([0.7376, 0.1948], device='cuda:0', requires_grad=True)), ('encoder.batch_norm2.bias', Parameter containing:\n",
      "tensor([0., 0.], device='cuda:0', requires_grad=True)), ('decoder.fc3.weight', Parameter containing:\n",
      "tensor([[-0.0394, -0.1805],\n",
      "        [-0.5127,  1.4429],\n",
      "        [-1.5748, -0.5489],\n",
      "        [-1.3788, -0.8488],\n",
      "        [-1.2493, -0.9131],\n",
      "        [-0.9361, -0.5044],\n",
      "        [-1.3268, -0.7889],\n",
      "        [ 1.4410, -0.5343]], device='cuda:0', requires_grad=True)), ('decoder.fc3.bias', Parameter containing:\n",
      "tensor([-0.1574,  0.8298,  0.2868,  0.4118,  0.4246,  0.1877,  0.3590,  0.3628],\n",
      "       device='cuda:0', requires_grad=True)), ('decoder.fc4.weight', Parameter containing:\n",
      "tensor([[ 4.7905e-01,  3.0370e-02, -2.6223e-01, -5.8308e-02, -1.3086e-01,\n",
      "         -6.5361e-02,  3.3976e-01, -1.3089e-01],\n",
      "        [ 2.1086e-01, -3.9071e-01,  1.3516e-01, -1.1588e-02, -5.2033e-02,\n",
      "          4.5875e-02, -3.8542e-02,  2.2226e-01],\n",
      "        [ 1.1785e-01, -6.7732e-01,  2.3643e-01,  1.6658e-01, -2.5466e-01,\n",
      "         -1.8235e-01,  1.0726e-01,  2.4390e-01],\n",
      "        [ 7.2778e-02, -3.4644e-01,  1.0875e-01, -2.7608e-01, -2.4033e-02,\n",
      "         -1.9565e-01,  4.2243e-01,  1.7053e-01],\n",
      "        [ 4.1022e-01, -4.1202e-01, -1.0148e-01,  8.1430e-02, -1.0576e-01,\n",
      "         -1.1495e-02,  1.2322e-01,  6.4268e-02],\n",
      "        [ 1.8244e-01, -7.7067e-02, -2.2568e-02, -3.7110e-01, -2.2411e-02,\n",
      "          1.5637e-01,  1.4564e-01, -1.0902e-01],\n",
      "        [ 4.5755e-01, -4.4820e-01,  2.0038e-01, -2.7367e-01, -1.6575e-01,\n",
      "         -7.0157e-03,  1.6285e-01,  9.6905e-02],\n",
      "        [ 1.4743e-01, -4.8190e-01,  2.1622e-02, -9.8213e-02, -2.7152e-01,\n",
      "          1.7485e-01,  4.0060e-02,  1.2385e-02],\n",
      "        [ 2.2845e-01, -5.4099e-03, -2.2669e-02, -2.6994e-01,  6.1429e-02,\n",
      "         -1.3868e-01,  2.3439e-01, -1.0573e-01],\n",
      "        [ 1.1892e-01, -2.2851e-01,  6.4230e-02,  6.1709e-02, -1.1433e-02,\n",
      "         -1.1173e-02, -1.7615e-01,  6.3275e-02],\n",
      "        [-1.0325e+01,  1.0692e+01, -9.9167e+00, -1.0264e+01, -9.9706e+00,\n",
      "         -1.0160e+01, -1.0079e+01,  1.0322e+01],\n",
      "        [ 1.6552e-01, -5.1506e-01, -3.5177e-02,  6.8255e-03, -2.1032e-01,\n",
      "         -1.4141e-01,  1.0415e-01,  9.0526e-01]], device='cuda:0',\n",
      "       requires_grad=True)), ('decoder.fc4.bias', Parameter containing:\n",
      "tensor([ 0.1233, -0.3489, -0.2060, -0.3716,  0.1334, -0.4064,  0.1352,  0.1065,\n",
      "        -0.2825, -0.0256,  8.9037,  0.1146], device='cuda:0',\n",
      "       requires_grad=True)), ('decoder.batch_norm3.weight', Parameter containing:\n",
      "tensor([0.5614, 2.0125, 3.3883, 3.0348, 3.3095, 3.2711, 2.8475, 2.0475],\n",
      "       device='cuda:0', requires_grad=True)), ('decoder.batch_norm3.bias', Parameter containing:\n",
      "tensor([-10.4195,  10.2017, -10.3758, -10.1711, -10.2935, -10.2255, -10.3405,\n",
      "         10.1626], device='cuda:0', requires_grad=True)), ('decoder.batch_norm4.weight', Parameter containing:\n",
      "tensor([0.0352, 0.9697, 0.8372, 0.8212, 0.4286, 0.1833, 0.2474, 0.0590, 0.1474,\n",
      "        0.8502, 0.1189, 0.1329], device='cuda:0', requires_grad=True)), ('decoder.batch_norm4.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "params = list(ae.named_parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ae, 'ae.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Encoder(\n",
      "    (fc1): Linear(in_features=12, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (batch_norm1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc3): Linear(in_features=2, out_features=8, bias=True)\n",
      "    (fc4): Linear(in_features=8, out_features=12, bias=True)\n",
      "    (batch_norm3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ae = torch.load('ae.pkl')\n",
    "print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Clustering",
   "language": "python",
   "name": "clustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
